{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0be2c143",
   "metadata": {},
   "source": [
    "# Testing Reliability of Prompting\n",
    "- Comparing logits with LLM answer\n",
    "- Given a specific answer how big is the corresponding token probability?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2122cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages and helpers\n",
    "import random \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "def randomized_choice_numbers():\n",
    "    possible_pairs = [[1,8], [2, 9], [8, 1], [9, 2]]\n",
    "    var_1, var_2 = random.choice(possible_pairs) \n",
    "    return var_1, var_2\n",
    "\n",
    "def randomized_prompt():\n",
    "    var1, var2 = randomized_choice_numbers()\n",
    "    remember_vars = [var1, var2]\n",
    "    prompt = f\"How do you see yourself: are you generally fully prepared to take risks or do you try to avoid taking risks? Please reply with a number between {var1} and {var2}. The value {var1} means: ‘not at all willing to take risks’ and the value {var2} means: ‘very willing to take risks’. Please give only the number between {var1} and {var2} that describes you best as your response. Answer: \"\n",
    "    return prompt, remember_vars\n",
    "\n",
    "def _has_chat_template(tok: AutoTokenizer) -> bool:\n",
    "    \"\"\"Check if tokenizer has a chat template.\"\"\"\n",
    "    try:\n",
    "        tmpl = getattr(tok, \"chat_template\", None)\n",
    "        return bool(tmpl)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def initialize_model_and_tokenizer(MODEL_NAME) -> bool:\n",
    "    \"\"\"Load the SmolLM model and tokenizer.\"\"\"\n",
    "    #global MODEL, TOKENIZER\n",
    "    try:\n",
    "        logging.info(f\"Loading model '{MODEL_NAME}'...\")\n",
    "        \n",
    "        MODEL = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        MODEL.eval()\n",
    "\n",
    "        TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        TOKENIZER.padding_side = 'right'\n",
    "        \n",
    "        # Set up padding token\n",
    "        if TOKENIZER.pad_token is None and TOKENIZER.eos_token is not None:\n",
    "            TOKENIZER.pad_token = TOKENIZER.eos_token\n",
    "        elif TOKENIZER.pad_token is None and TOKENIZER.eos_token is None:\n",
    "            TOKENIZER.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "            MODEL.resize_token_embeddings(len(TOKENIZER))\n",
    "\n",
    "        logging.info(\"Model and tokenizer loaded successfully. Chat template detected: %s\",\n",
    "                     _has_chat_template(TOKENIZER))\n",
    "        return MODEL, TOKENIZER\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load model '{MODEL_NAME}'. Error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846589c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_names = [\"HuggingFaceTB/SmolLM2-1.7B-Instruct\", \"unsloth/Qwen3-1.7B\"],# \"unsloth/Qwen3-4B\",  \"bigscience/bloomz-1b1\", \"bigscience/bloomz-3b\", \"tiiuae/falcon-7b-instruct\"]\n",
    "short_model_names = [\"SmolLM2-1.7B-I\", \"Qwen3-1.7B-I\",\"Qwen3-4B-I\" ,\"bloomz-1b1\", \"bloomz-3b\", \"falcon-7b-i\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a3f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to load model '['HuggingFaceTB/SmolLM2-1.7B-Instruct', 'unsloth/Qwen3-1.7B']'. Error: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '['HuggingFaceTB/SmolLM2-1.7B-Instruct', 'unsloth/Qwen3-1.7B']'. Use `repo_type` argument if needed.\n",
      "WARNING:root:Skipping model ['HuggingFaceTB/SmolLM2-1.7B-Instruct', 'unsloth/Qwen3-1.7B'] due to error: cannot unpack non-iterable bool object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_data_in_table_list = []\n",
    "\n",
    "for short_model_name, MODEL_NAME in zip(short_model_names, model_names):\n",
    "\ttry:\n",
    "\t\tMODEL, TOKENIZER = initialize_model_and_tokenizer(MODEL_NAME) # Initialize model and tokenizer\n",
    "\t\tif not MODEL: \n",
    "\t\t\tlogging.warning(f\"Skipping model {MODEL_NAME} due to load failure\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tprint(f\"Processing model {MODEL_NAME}...\")\n",
    "        \n",
    "\n",
    "\t\tfor i in range(10):\n",
    "\t\t\t# generate prompt with random number combi\n",
    "\t\t\tprompt, remember_vars  = randomized_prompt()\n",
    "\n",
    "\t\t\t# Model specific chat template for prompting\n",
    "\t\t\tmessages = [\n",
    "\t\t\t\t{\"role\": \"user\", \"content\": prompt},\n",
    "\t\t\t]\n",
    "\n",
    "\t\t\t# Tokenize input\n",
    "\t\t\tif _has_chat_template(TOKENIZER):\n",
    "\t\t\t\tinputs = TOKENIZER.apply_chat_template(\n",
    "\t\t\t\t\tmessages,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=True,\n",
    "                    return_dict=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    skip_special_tokens=True,\n",
    "                    enable_thinking=False\n",
    "\t\t\t\t).to(MODEL.device)\n",
    "\t\t\telse:\n",
    "\t\t\t\tinputs = TOKENIZER(\n",
    "\t\t\t\t\t[msg[\"content\"] for msg in messages],\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True\n",
    "\t\t\t\t).to(MODEL.device)\n",
    "\n",
    "\t\t\t# generate output text and print\n",
    "\t\t\tgenerated_outputs = MODEL.generate(**inputs, max_new_tokens=40)\n",
    "\t\t\tgenerated_text = MODEL.decode(\n",
    "\t\t\t\tgenerated_outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "\t\t\t\t\tskip_special_tokens=True)\n",
    "\n",
    "\t\t\t#print(\"Generated text:\", generated_text)\n",
    "\n",
    "\t\t\t# generate output logits (from inner last layer of model)\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tlogit_outputs = MODEL(**inputs)\n",
    "\t\t\t\tlogits = logit_outputs.logits\n",
    "\n",
    "\t\t\t# Take the logits for the *last* position in the prompt\n",
    "\t\t\tnext_token_logits = logits[0, -1, :]\n",
    "\n",
    "\t\t\t# Convert to probabilities\n",
    "\t\t\tprobs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "\t\t\t# Extract most likely token (= max logit value) and its prob (i.e. softmax function = exp of logits and divided by sum of all expnential logits so that probs sum up to)\n",
    "\t\t\tpredicted_id = torch.argmax(probs).item()\n",
    "\t\t\tpredicted_token = TOKENIZER.decode(predicted_id)\n",
    "\t\t\tpredicted_prob = probs[predicted_id].item()\n",
    "\n",
    "\t\t\t# Extract the *actual generated* token and its prob at first response position!\n",
    "\t\t\t# Take only the first token\n",
    "\t\t\tgenerated_tokens = generated_outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "\t\t\tfirst_generated_id = generated_tokens[0].item()\n",
    "\t\t\tactual_token = TOKENIZER.decode([first_generated_id], skip_special_tokens=True)\n",
    "\t\t\tactual_prob = probs[first_generated_id].item()\n",
    "\n",
    "\t\t\t# Add probabilities for 1-9\n",
    "\t\t\t# Get token IDs for numbers 1–9\n",
    "\t\t\ttoken_ids = [TOKENIZER.encode(str(i), add_special_tokens=False)[0] for i in range(1, 10)]\n",
    "\t\t\t# Extract their probabilities\n",
    "\t\t\tselected_probs = probs[token_ids]\n",
    "\t\t\t# Decode tokens for display\n",
    "\t\t\ttokens = [TOKENIZER.decode([tid]) for tid in token_ids]\n",
    "\t\t\tprobs_list = selected_probs.tolist()\n",
    "\n",
    "\t\t\tsave_data_in_table_list.append(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"model\": short_model_name,\n",
    "\t\t\t\t\t\"prompt_numbers\": remember_vars,\n",
    "\t\t\t\t\t\"predicted_token\": predicted_token,\n",
    "\t\t\t\t\t\"predicted_prob\": predicted_prob,\n",
    "\t\t\t\t\t\"actual_token\": actual_token,\n",
    "\t\t\t\t\t\"actual_prob\": actual_prob,\n",
    "\t\t\t\t\t\"Token\": tokens,\n",
    "\t\t\t\t\t\"Probability\": probs_list\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n",
    "\t\t# After finishing work with the model\n",
    "\t\tprint(f\"Model {MODEL_NAME} done!\")\n",
    "\t\tdel MODEL\n",
    "\t\tdel TOKENIZER\n",
    "\t\t# Run garbage collector\n",
    "\t\tgc.collect()\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tlogging.warning(f\"Skipping model {MODEL_NAME} due to error: {e}\")\n",
    "\t\tcontinue\n",
    "\n",
    "\n",
    "\t\t\n",
    "\n",
    "df = pd.DataFrame(save_data_in_table_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b8261dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prompt_numbers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mprompt_tuple\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_numbers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mtuple\u001b[39m(x))\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Loop over each model\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model, model_df \u001b[38;5;129;01min\u001b[39;00m df.groupby(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Get unique prompt_numbers within this model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/projectenv/lib/python3.11/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/projectenv/lib/python3.11/site-packages/pandas/core/indexes/range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'prompt_numbers'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df[\"prompt_tuple\"] = df[\"prompt_numbers\"].apply(lambda x: tuple(x))\n",
    "\n",
    "# Loop over each model\n",
    "for model, model_df in df.groupby(\"model\"):\n",
    "    \n",
    "    # Get unique prompt_numbers within this model\n",
    "    prompts = model_df[\"prompt_tuple\"].unique()\n",
    "    n_prompts = len(prompts)\n",
    "\n",
    "    # Create one column per prompt_numbers\n",
    "    fig, axes = plt.subplots(1, n_prompts, figsize=(5*n_prompts, 5), sharey=True)\n",
    "\n",
    "    # If only 1 prompt, axes is not a list\n",
    "    if n_prompts == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, prompt in zip(axes, prompts):\n",
    "        subset = model_df[model_df[\"prompt_tuple\"] == prompt]\n",
    "\n",
    "        # Assume all rows have the same tokens order\n",
    "        tokens = subset.iloc[0][\"Token\"]\n",
    "        x = np.arange(len(tokens))\n",
    "        width = 0.8 / len(subset)   # distribute bars within each token group\n",
    "\n",
    "        # Plot each run (row) as a differently colored bar\n",
    "        for i, (_, row) in enumerate(subset.iterrows()):\n",
    "            probs = row[\"Probability\"]\n",
    "            ax.bar(x + i*width, probs, width, label=f\"Run {i+1}\")\n",
    "\n",
    "        ax.set_title(f\"Prompt: {prompt}\")\n",
    "        ax.set_xticks(x + width*(len(subset)-1)/2)\n",
    "        ax.set_xticklabels(tokens)\n",
    "        ax.set_xlabel(\"Token\")\n",
    "        ax.set_ylabel(\"Probability\")\n",
    "        ax.legend()\n",
    "\n",
    "    fig.suptitle(f\"Model: {model}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
